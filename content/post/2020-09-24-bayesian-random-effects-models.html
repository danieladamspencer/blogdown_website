---
title: Bayesian Random Effects Models
author: "Dan"
date: '2020-09-24'
slug: bayesian-random-effects-models
categories: []
tags: []
---



<p>Consider a scenario in which noisy data are observed, with variance driven by two primary sources: <strong>signal</strong> and <strong>noise</strong>. We can write the data generation as</p>
<p><span class="math display">\[ W_{ij} = X_i + E_{ij}, \quad i = 1,\ldots, n, \quad j = 1,2 \]</span></p>
<p>Here, consider <span class="math inline">\(X_i\)</span> as the <strong>signal</strong> and <span class="math inline">\(E_{ij}\)</span> as the <strong>noise</strong>, with values generated from zero-mean normal distributions.</p>
<p><span class="math display">\[ X_i \sim \text{Normal}(0,\sigma_x^2), \quad E_{ij} \sim \text{Normal}(0,\sigma_e^2) \]</span></p>
<pre class="r"><code>set.seed(47403) # Set this so the results are always the same
sigma2_e_true &lt;- 3
sigma2_x_true &lt;- 5
n &lt;- 100
E &lt;- matrix(rnorm(n*2, sd = sqrt(sigma2_e_true)),n,2)
X &lt;- rnorm(n,sd = sqrt(sigma2_x_true))
W &lt;- cbind(X,X) + E</code></pre>
<p>If we want to solve for the variance parameters <span class="math inline">\(\sigma_x^2\)</span> and <span class="math inline">\(\sigma_e^2\)</span>, as Bayesians we could give each parameter a prior and look at the posterior distributions of both. For convenience, give both parameters a relatively uninformative inverse gamma prior (for conjugacy).</p>
<p><span class="math display">\[ \sigma_x^2, \sigma_e^2 \overset{iid}{\sim} \text{Inverse Gamma} (0.1, 1)\]</span>
Now that we have the full model, we can find that the posterior full-conditional distributions are relatively easy to come across:</p>
<p><span class="math display">\[X_i | W_{i\cdot},\sigma_e^2, \sigma_x^2 \sim \text{Normal}\left( \left( \frac{2}{\sigma_x^2} + \frac{1}{\sigma_e^2} \right)^{-1} \frac{\sum_{j=1}^2 W_{ij}}{\sigma_e^2}, \left( \frac{2}{\sigma_x^2} + \frac{1}{\sigma_e^2} \right)^{-1} \right),\]</span></p>
<p><span class="math display">\[\sigma_x^2 | X_{\cdot} \sim \text{Inverse Gamma} \left( \frac{n}{2} + 0.1, 1 + \frac{1}{2}\sum_{i=1}^{n} X_i^2 \right),\]</span></p>
<p><span class="math display">\[\sigma_e^2 | X_\cdot, W_{\cdot\cdot} \sim \text{Inverse Gamma}\left( n + 0.1, 1 + \frac{1}{2} \sum_{i = 1}^n \sum_{j = 1}^2 (W_{ij} - X_i)^2 \right).\]</span></p>
<p>Without doing much more work, we can sample from the posterior distribution using MCMC and make some inference! So how does it do?</p>
<pre class="r"><code># W is the observed data and S is the number of samples to draw from the 
# posterior distribution.
mcmc_function &lt;- function(W, S = 10000, a_sigma_e = 0.1, b_sigma_e = 1, 
                          a_sigma_x = 0.1, b_sigma_x = 1, B = 100) {
  # Start with initial conditions
  sigma2_e &lt;- mean(apply(W,1,var))
  sigma2_x &lt;- var(apply(W,1,mean))
  X &lt;- apply(W,1,mean)
  
  # Preallocate storage for draws from the posterior distribution
  mcmc_results &lt;- list(
    X = matrix(NA,n,S),
    sigma2_e = numeric(S),
    sigma2_x = numeric(S)
  )
  
  # Run the MCMC for 10k iterations
  start_time &lt;- proc.time()[3]
  for(s in seq(S)) {
    X &lt;- rnorm(n, (1 / (1/sigma2_e + 2/sigma2_x)) * apply(W,1,sum)/sigma2_e, 
               sd = sqrt((1 / (1/sigma2_e + 2/sigma2_x))))
    sigma2_x &lt;- 1/rgamma(1,n/2 + a_sigma_x, b_sigma_x + sum(X^2)/2)
    sigma2_e &lt;- 1/rgamma(1,n + a_sigma_e, b_sigma_e + sum(apply(W,2,`-`,y =X)^2)/2)
    mcmc_results$X[,s] &lt;- X
    mcmc_results$sigma2_e[s] &lt;- sigma2_e
    mcmc_results$sigma2_x[s] &lt;- sigma2_x
  }
  total_time &lt;- proc.time()[3] - start_time
  cat(&quot;Total MCMC run time:&quot;,S,&quot;draws in&quot;,total_time,&quot;seconds \n&quot;)
  # Remove burn-in
  mcmc_results$X &lt;- mcmc_results$X[,-seq(B)]
  mcmc_results$sigma2_e &lt;- mcmc_results$sigma2_e[-seq(B)]
  mcmc_results$sigma2_x &lt;- mcmc_results$sigma2_x[-seq(B)]
  return(mcmc_results)
}
mcmc_results &lt;- mcmc_function(W)</code></pre>
<pre><code>## Total MCMC run time: 10000 draws in 1.839 seconds</code></pre>
<p>Itâ€™s always a good idea to see how the draws from the posterior distribution vary over the draws that are made, as there can be a problem if there is too much dependence between one draw and the next. These types of plots are referred to as <em>trace plots</em>.</p>
<pre class="r"><code>par(mfrow=c(2,1), mar = c(5,5,1,1))
plot(mcmc_results$sigma2_e, type=&#39;l&#39;, main = &quot;&quot;, ylab = expression(sigma[e]^2), xlab = &quot;Iteration&quot;)
plot(mcmc_results$sigma2_x, type=&#39;l&#39;, main = &quot;&quot;, ylab = expression(sigma[x]^2), xlab = &quot;Iteration&quot;)</code></pre>
<p><img src="/post/2020-09-24-bayesian-random-effects-models_files/figure-html/trace%20plots-1.png" width="672" /></p>
<p>These plots look good, as the values for the parameters very quickly adhere around a horizontal line without a strong discernable trend. We can look at the density plots for the posterior draws of <span class="math inline">\(\sigma_x^2\)</span> and <span class="math inline">\(\sigma_e^2\)</span> and see how close they were to the true values:</p>
<pre class="r"><code>par(mfrow=c(1,2))
plot(density(mcmc_results$sigma2_e), main = &quot;&quot;, xlab = expression(sigma[e]^2))
abline(v = sigma2_e_true, col = &#39;red&#39;)
plot(density(mcmc_results$sigma2_x), main = &quot;&quot;, xlab = expression(sigma[x]^2))
abline(v = sigma2_x_true, col = &#39;red&#39;)</code></pre>
<p><img src="/post/2020-09-24-bayesian-random-effects-models_files/figure-html/posterior%20density%20plots-1.png" width="672" /></p>
<p>What happened here? It looks almost like the posterior draws for the two variance parameters bounced closer to the values of the opposite parameter! This is a problem know as <strong>identifiability</strong>, in which two values are being summed or multiplied to account for the same effect. In this case, both <span class="math inline">\(\sigma_x^2\)</span> and <span class="math inline">\(\sigma_e^2\)</span> are used to account for variance in <span class="math inline">\(\mathbf{W}\)</span>. What happens if <span class="math inline">\(\sigma_x^2 = 100\)</span> and <span class="math inline">\(\sigma_e^2 = 1\)</span>?</p>
<pre class="r"><code>set.seed(47403) # Set this so the results are always the same
sigma2_e_true &lt;- 1
sigma2_x_true &lt;- 100
n &lt;- 100
E &lt;- matrix(rnorm(n*2, sd = sqrt(sigma2_e_true)),n,2)
X &lt;- rnorm(n,sd = sqrt(sigma2_x_true))
W &lt;- cbind(X,X) + E

mcmc_results2 &lt;- mcmc_function(W)</code></pre>
<pre><code>## Total MCMC run time: 10000 draws in 1.746 seconds</code></pre>
<pre class="r"><code>par(mfrow=c(1,2))
plot(density(mcmc_results2$sigma2_e), main = &quot;&quot;, xlab = expression(sigma[e]^2))
abline(v = sigma2_e_true, col = &#39;red&#39;)
plot(density(mcmc_results2$sigma2_x), main = &quot;&quot;, xlab = expression(sigma[x]^2))
abline(v = sigma2_x_true, col = &#39;red&#39;)</code></pre>
<p><img src="/post/2020-09-24-bayesian-random-effects-models_files/figure-html/second%20mcmc-1.png" width="672" /></p>
<p>We see from these results that the difference bouys the posterior draws for <span class="math inline">\(\sigma_x^2\)</span> upward, to the point that most of the marginal posterior density is higher than the true value. This increase in variance leads to high draws for <span class="math inline">\(X_i\)</span>, resulting in higher draws for <span class="math inline">\(\sigma_e^2\)</span>. What happens when the sample size is increased from 100 to 1,000?</p>
<pre class="r"><code>set.seed(47403) # Set this so the results are always the same
sigma2_e_true &lt;- 1
sigma2_x_true &lt;- 100
n &lt;- 1000
E &lt;- matrix(rnorm(n*2, sd = sqrt(sigma2_e_true)),n,2)
X &lt;- rnorm(n,sd = sqrt(sigma2_x_true))
W &lt;- cbind(X,X) + E

mcmc_results3 &lt;- mcmc_function(W)</code></pre>
<pre><code>## Total MCMC run time: 10000 draws in 13.578 seconds</code></pre>
<pre class="r"><code>par(mfrow=c(1,2))
plot(density(mcmc_results3$sigma2_e), main = &quot;&quot;, xlab = expression(sigma[e]^2))
abline(v = sigma2_e_true, col = &#39;red&#39;)
plot(density(mcmc_results3$sigma2_x), main = &quot;&quot;, xlab = expression(sigma[x]^2))
abline(v = sigma2_x_true, col = &#39;red&#39;)</code></pre>
<p><img src="/post/2020-09-24-bayesian-random-effects-models_files/figure-html/third%20mcmc-1.png" width="672" /></p>
<p>While there is less skew in the marginal posterior distributions, the inflation of the variance parameters has only gotten worse! A Bayesian might wonder if the priors may be playing a part in this strange switching. After all, the mean of an inverse gamma distribution with shape 0.1 and rate 1 is technically infinite. What if the priors were more informative, say with means around 1 and 100 for <span class="math inline">\(\sigma_e^2\)</span> and <span class="math inline">\(\sigma_x^2\)</span>, respectively?</p>
<pre class="r"><code>set.seed(47403) # Set this so the results are always the same
sigma2_e_true &lt;- 1
sigma2_x_true &lt;- 100
n &lt;- 100
E &lt;- matrix(rnorm(n*2, sd = sqrt(sigma2_e_true)),n,2)
X &lt;- rnorm(n,sd = sqrt(sigma2_x_true))
W &lt;- cbind(X,X) + E

mcmc_results4 &lt;- mcmc_function(W, a_sigma_e = 2, b_sigma_e = 2, a_sigma_x = 2, b_sigma_x = 200)</code></pre>
<pre><code>## Total MCMC run time: 10000 draws in 1.652 seconds</code></pre>
<pre class="r"><code>par(mfrow=c(1,2))
plot(density(mcmc_results4$sigma2_e), main = &quot;&quot;, xlab = expression(sigma[e]^2))
abline(v = sigma2_e_true, col = &#39;red&#39;)
plot(density(mcmc_results4$sigma2_x), main = &quot;&quot;, xlab = expression(sigma[x]^2))
abline(v = sigma2_x_true, col = &#39;red&#39;)</code></pre>
<p><img src="/post/2020-09-24-bayesian-random-effects-models_files/figure-html/fourth%20mcmc-1.png" width="672" /></p>
<p>This shows that the influence of the prior distributions appears to be negligible. What does increasing the sample size back to 1000 have in this case?</p>
<pre class="r"><code>set.seed(47403) # Set this so the results are always the same
sigma2_e_true &lt;- 1
sigma2_x_true &lt;- 100
n &lt;- 1000
E &lt;- matrix(rnorm(n*2, sd = sqrt(sigma2_e_true)),n,2)
X &lt;- rnorm(n,sd = sqrt(sigma2_x_true))
W &lt;- cbind(X,X) + E

mcmc_results5 &lt;- mcmc_function(W, a_sigma_e = 2, b_sigma_e = 2, a_sigma_x = 2, b_sigma_x = 200)</code></pre>
<pre><code>## Total MCMC run time: 10000 draws in 13.448 seconds</code></pre>
<pre class="r"><code>par(mfrow=c(1,2))
plot(density(mcmc_results5$sigma2_e), main = &quot;&quot;, xlab = expression(sigma[e]^2))
abline(v = sigma2_e_true, col = &#39;red&#39;)
plot(density(mcmc_results5$sigma2_x), main = &quot;&quot;, xlab = expression(sigma[x]^2))
abline(v = sigma2_x_true, col = &#39;red&#39;)</code></pre>
<p><img src="/post/2020-09-24-bayesian-random-effects-models_files/figure-html/fifth%20mcmc-1.png" width="672" /></p>
<p>Since the data now plays a stronger role, the priors have even less of an effect, variance inflation is stronger here. What if the parameters themselves are switched? That is, what if the observation variance <span class="math inline">\(\sigma_e^2\)</span> and the effect variance <span class="math inline">\(\sigma_x^2\)</span> are set such that the effect variance is smaller than the observation variance?</p>
<pre class="r"><code>set.seed(47403) # Set this so the results are always the same
sigma2_e_true &lt;- 100
sigma2_x_true &lt;- 1
n &lt;- 100
E &lt;- matrix(rnorm(n*2, sd = sqrt(sigma2_e_true)),n,2)
X &lt;- rnorm(n,sd = sqrt(sigma2_x_true))
W &lt;- cbind(X,X) + E

mcmc_results6 &lt;- mcmc_function(W, a_sigma_e = 2, b_sigma_e = 200, a_sigma_x = 2, b_sigma_x = 2)</code></pre>
<pre><code>## Total MCMC run time: 10000 draws in 1.811 seconds</code></pre>
<pre class="r"><code>par(mfrow=c(1,2))
plot(density(mcmc_results6$sigma2_e), main = &quot;&quot;, xlab = expression(sigma[e]^2))
abline(v = sigma2_e_true, col = &#39;red&#39;)
plot(density(mcmc_results6$sigma2_x), main = &quot;&quot;, xlab = expression(sigma[x]^2))
abline(v = sigma2_x_true, col = &#39;red&#39;)</code></pre>
<p><img src="/post/2020-09-24-bayesian-random-effects-models_files/figure-html/sixth%20mcmc-1.png" width="672" /></p>
<p>Now we can see that the posterior inference is reflective of the true values, though the variance for the random effect is dramatically shrunken towards zero. What does this mean? In cases where there are random effects with mean 0, the largest source of variance gets pooled into the observation variance while the smaller variance is reserved for the random effect variance. Another test may shed some light here. What if the mean of the random effect is sufficiently large relative to the observation variance, say 50?</p>
<pre class="r"><code>set.seed(47403) # Set this so the results are always the same
sigma2_e_true &lt;- 100
sigma2_x_true &lt;- 1
n &lt;- 100
E &lt;- matrix(rnorm(n*2, sd = sqrt(sigma2_e_true)),n,2)
X &lt;- rnorm(n,mean = 50, sd = sqrt(sigma2_x_true))
W &lt;- cbind(X,X) + E

mcmc_results7 &lt;- mcmc_function(W, a_sigma_e = 2, b_sigma_e = 200, a_sigma_x = 2, b_sigma_x = 2)</code></pre>
<pre><code>## Total MCMC run time: 10000 draws in 1.916 seconds</code></pre>
<pre class="r"><code>par(mfrow=c(1,2))
plot(density(mcmc_results7$sigma2_e), main = &quot;&quot;, xlab = expression(sigma[e]^2))
abline(v = sigma2_e_true, col = &#39;red&#39;)
plot(density(mcmc_results7$sigma2_x), main = &quot;&quot;, xlab = expression(sigma[x]^2))
abline(v = sigma2_x_true, col = &#39;red&#39;)</code></pre>
<p><img src="/post/2020-09-24-bayesian-random-effects-models_files/figure-html/seventh%20mcmc-1.png" width="672" /></p>
<p>There has been almost no change in the posterior distribution of <span class="math inline">\(\sigma_x^2\)</span>. but the estimate for <span class="math inline">\(\sigma_e^2\)</span> is huge! Clearly there is something happening here that is</p>
<pre class="r"><code>set.seed(47403) # Set this so the results are always the same
sigma2_e_true &lt;- 100
sigma2_x_true &lt;- 1
n &lt;- 100
E &lt;- matrix(rnorm(n*2, sd = sqrt(sigma2_e_true)),n,2)
X &lt;- rnorm(n,mean = 1000, sd = sqrt(sigma2_x_true))
W &lt;- cbind(X,X) + E

mcmc_results8 &lt;- mcmc_function(W, a_sigma_e = 2, b_sigma_e = 200, a_sigma_x = 2, b_sigma_x = 2)</code></pre>
<pre><code>## Total MCMC run time: 10000 draws in 1.97 seconds</code></pre>
<pre class="r"><code>par(mfrow=c(1,2))
plot(density(mcmc_results8$sigma2_e), main = &quot;&quot;, xlab = expression(sigma[e]^2))
abline(v = sigma2_e_true, col = &#39;red&#39;)
plot(density(mcmc_results8$sigma2_x), main = &quot;&quot;, xlab = expression(sigma[x]^2))
abline(v = sigma2_x_true, col = &#39;red&#39;)</code></pre>
<p><img src="/post/2020-09-24-bayesian-random-effects-models_files/figure-html/eighth%20mcmc-1.png" width="672" /></p>
<p>This is certainly something to keep in mind when designing your models and especially when attempting to model variance using Bayesian methods!</p>
